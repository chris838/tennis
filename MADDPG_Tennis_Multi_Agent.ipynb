{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb, pickle, torch, random\n",
    "\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from holoviews import opts\n",
    "from holoviews.streams import Pipe, Buffer\n",
    "from holoviews.operation.timeseries import rolling\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "from maddpg_agent import MaddpgAgent\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name=\"./Tennis.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default brain and reset env\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# Number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print(f\"Number of agents: {num_agents}\")\n",
    "\n",
    "# Size of the global state/action space (across all agents)\n",
    "actions = env_info.previous_vector_actions\n",
    "states = env_info.vector_observations\n",
    "global_state_space_size = states.flatten().shape[0]\n",
    "global_action_space_size = actions.flatten().shape[0]\n",
    "print(f\"Global states: {global_state_space_size}\")\n",
    "print(f\"Global actions: {global_action_space_size}\")\n",
    "\n",
    "# Size of the local state/action space (for each agent individually)\n",
    "action_space_size = brain.vector_action_space_size\n",
    "state_space_size = brain.num_stacked_vector_observations * brain.vector_observation_space_size\n",
    "print(f\"Local states: {state_space_size}\")\n",
    "print(f\"Local actions: {action_space_size}\")\n",
    "\n",
    "# Examine the state space \n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self,\n",
    "        train_after_samples = 20,\n",
    "        replay_buffer_size_max = int(1e6)\n",
    "                ):\n",
    "        self.train_after_samples = train_after_samples\n",
    "        self.replay_buffer_size_max = replay_buffer_size_max\n",
    "        \n",
    "        # Create the agents\n",
    "        self.agents = []\n",
    "        for i in range(num_agents):\n",
    "            print(f\"Agent {i}: state space: {state_space_size}; \\\n",
    "                    action space {action_space_size}.\")\n",
    "            self.agents.append(MaddpgAgent(\n",
    "                i, num_agents, state_space_size, action_space_size,\n",
    "                global_state_space_size, global_action_space_size))\n",
    "    \n",
    "        # Create the replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(\n",
    "                max_size=replay_buffer_size_max, min_samples_required=train_after_samples)\n",
    "            \n",
    "        # Track progres\n",
    "        self.episode_returns = []\n",
    "        self.loss = []\n",
    "        \n",
    "        # Training vars\n",
    "        self.train_step = 0\n",
    "        self.episode = 0\n",
    "        self.is_learning = False\n",
    "\n",
    "    def train(self,\n",
    "        num_episodes = 1500,\n",
    "        batch_size = 1024,\n",
    "        max_episode_length = 250,\n",
    "        train_every_steps = 100,\n",
    "        noise_level = 2.0,\n",
    "        noise_decay = 0.9999,\n",
    "        print_episodes = 100\n",
    "             ):\n",
    "        \n",
    "        try:\n",
    "\n",
    "            print(f\"------------------------------------------------\")\n",
    "            print(f\"New training run.\")\n",
    "            print(f\"    num_episodes: {num_episodes}\")\n",
    "            print(f\"    batch_size: {batch_size}\")\n",
    "            print(f\"    max_episode_length: {max_episode_length}\")\n",
    "            print(f\"    train_after_samples: {self.train_after_samples}\")\n",
    "            print(f\"    replay_buffer_size_max: {self.replay_buffer_size_max}\")\n",
    "            print(f\"    train_every_steps: {train_every_steps}\")\n",
    "            print(f\"    noise_level: {noise_level}\")\n",
    "            print(f\"    noise_decay: {noise_decay}\")\n",
    "\n",
    "            # Iterate over episodes\n",
    "            episode_max = self.episode + num_episodes\n",
    "            while self.episode < episode_max:\n",
    "\n",
    "                # Receive initial state vector s\n",
    "                #   s = (s_1, . . . , s_N)\n",
    "                env_info = env.reset(train_mode=True)[brain_name]\n",
    "                s = env_info.vector_observations \n",
    "\n",
    "                self.episode_returns.append( np.array( [0] * num_agents) )\n",
    "                for t in range(1, max_episode_length):\n",
    "\n",
    "                    # For each agent i, select actions:\n",
    "                    #   a = (a_1, . . . , a_N)\n",
    "                    # using the current policy and exploration noise, which we decay\n",
    "                    a = [agent.act(state, noise_level=noise_level)\n",
    "                         for agent, state in zip(self.agents, s)]\n",
    "                    if self.is_learning:\n",
    "                        noise_level *= noise_decay\n",
    "\n",
    "                    # Execute actions a = (a_1, . . . , a_N)\n",
    "                    # Observe:\n",
    "                    #   Reward r = (r_1, . . . , r_N)\n",
    "                    #   Next-state vector s' = (s'_1, . . . , s'_N)\n",
    "                    env_info= env.step(a)[brain_name]\n",
    "                    r = env_info.rewards\n",
    "                    s_prime = env_info.vector_observations\n",
    "                    dones = env_info.local_done\n",
    "\n",
    "                    # Store (s, a, r, s', done) in replay buffer\n",
    "                    self.replay_buffer.append((s, a, r, s_prime, dones))\n",
    "\n",
    "                    # Record progress\n",
    "                    self.episode_returns[-1] = self.episode_returns[-1] + r\n",
    "\n",
    "                    # Advance\n",
    "                    s = s_prime\n",
    "                    self.train_step += 1\n",
    "\n",
    "                    # Periodically (after a certain number of steps) run update/training\n",
    "                    if self.train_step % train_every_steps == 0:\n",
    "                        if self.replay_buffer.has_enough_samples():\n",
    "\n",
    "                            if not self.is_learning:\n",
    "                                print(f\"Started learning at time {self.train_step}\")\n",
    "                                self.is_learning = True\n",
    "\n",
    "                            # Sample replay buffer\n",
    "                            sample = self.replay_buffer.sample(batch_size=batch_size)\n",
    "\n",
    "                            # For every sample tuple, each agent needs to know which action\n",
    "                            # would be chosen under the policy of the other agents in the\n",
    "                            # next state s', in order to calculate q-values.\n",
    "                            next_actions = [[\n",
    "                                 agent.act(next_state, target_actor=True)\n",
    "                                 for agent, next_state in zip(self.agents, s_prime)]\n",
    "                                for (s, a, r, s_prime, dones) in sample]\n",
    "\n",
    "                            # Update/train all the agents\n",
    "                            per_agent_loss = []\n",
    "                            for agent in self.agents:\n",
    "                                actor_loss, critic_loss = agent.update(sample, next_actions)\n",
    "                                per_agent_loss.append((actor_loss, critic_loss))\n",
    "                            self.loss.append(per_agent_loss)\n",
    "\n",
    "                    # Terminate episode early if done\n",
    "                    if any(dones):\n",
    "                        break\n",
    "\n",
    "                self.episode += 1\n",
    "                if self.episode % print_episodes == 0:\n",
    "                    print(f\"t: {self.train_step}, e: {self.episode}, noise: {noise_level:.2f}. \" + \\\n",
    "                          f\"Average last {print_episodes} episode return: \" + \\\n",
    "                          f\"{np.array(self.episode_returns[-print_episodes:]).mean(axis=0)}\")\n",
    "\n",
    "            print(\"Finished\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted\")\n",
    "\n",
    "    def get_average_loss(self):\n",
    "        if len(self.loss) > 0:\n",
    "            return np.array(self.loss).mean(axis=1)\n",
    "        return [[0, 0]]\n",
    "    \n",
    "    def get_max_returns(self):\n",
    "        if len(self.episode_returns) > 0:\n",
    "            return np.array(self.episode_returns).max(axis=1)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "### Create a new trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "trainer = Trainer(\n",
    "    train_after_samples=40000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start/resume training sesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    num_episodes=15000,\n",
    "    batch_size=512,\n",
    "    train_every_steps=4,\n",
    "    noise_level = 1.0,\n",
    "    noise_decay = 0.9999,\n",
    "    max_episode_length = 250,\n",
    "    print_episodes = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display returns\n",
    "max_returns = trainer.get_max_returns()\n",
    "raw_returns = hv.Curve(max_returns, 'Episode', 'Return').relabel('Single episode')\n",
    "smooth_returns = rolling(hv.Curve(\n",
    "    max_returns, 'Episode', 'Return'), rolling_window=100).relabel('100 episode average')\n",
    "max_returns_curve = (raw_returns * smooth_returns).relabel('Max episode return')\n",
    "\n",
    "# Display loss\n",
    "average_loss = trainer.get_average_loss()\n",
    "actor_loss = hv.Curve(average_loss[:,0], 'Training iteration', 'Loss').relabel('Actor')\n",
    "critic_loss = hv.Curve(average_loss[:,1], 'Training iteration', 'Loss').relabel('Critic')\n",
    "loss_curves = (actor_loss * critic_loss).relabel('Actor/critic loss')\n",
    "\n",
    "(max_returns_curve + loss_curves).opts(opts.Curve(axiswise=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/restore training state\n",
    "\n",
    "### Pausing/resuming training progress\n",
    "\n",
    "This is especially useful because the Unity environment handle will be corrupted if you interrupt whilst training. Simply save the trainer, restart the kernel and unity environment, then load your progress to resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trainer to disk\n",
    "pickle.dump( trainer, open( \"saved_models/trainer.pickle\", \"wb\" ) )\n",
    "\n",
    "# Save torch params to file\n",
    "for i, agent in enumerate(trainer.agents):\n",
    "    torch.save(agent.actor_optimiser,   f\"saved_models/agent_{i}_actor_optimiser.pt\")\n",
    "    torch.save(agent.critic_optimiser,  f\"saved_models/agent_{i}_critic_optimiser.pt\")\n",
    "    torch.save(agent.actor,         f\"saved_models/agent_{i}_actor_model.pt\")\n",
    "    torch.save(agent.actor_target,  f\"saved_models/agent_{i}_actor_target_model.pt\")\n",
    "    torch.save(agent.critic,        f\"saved_models/agent_{i}_critic_model.pt\")\n",
    "    torch.save(agent.critic_target, f\"saved_models/agent_{i}_critic_target_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trainer from disk\n",
    "trainer = pickle.load( open( \"saved_models/trainer.pickle\", \"rb\" ) )\n",
    "\n",
    "# Load torch params from file (NOT safe across refactors)\n",
    "for i, agent in enumerate(trainer.agents):\n",
    "    agent.actor_optimiser  = torch.load(f\"saved_models/agent_{i}_actor_optimiser.pt\")\n",
    "    agent.critic_optimiser = torch.load(f\"saved_models/agent_{i}_critic_optimiser.pt\")\n",
    "    agent.actor         = torch.load(f\"saved_models/agent_{i}_actor_model.pt\")\n",
    "    agent.actor_target  = torch.load(f\"saved_models/agent_{i}_actor_target_model.pt\")\n",
    "    agent.critic        = torch.load(f\"saved_models/agent_{i}_critic_model.pt\")\n",
    "    agent.critic_target = torch.load(f\"saved_models/agent_{i}_critic_target_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watch agent play\n",
    "\n",
    "To view random play according to the OU noise process, set the noise level to 1. This is what we use to generate exploratory behaviour initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    t = 0\n",
    "    while True:\n",
    "        t += 1\n",
    "\n",
    "        actions = [agent.act(state, noise_level=0.5) for agent, state in zip(trainer.agents, states)]\n",
    "        \n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print(f'Episode: {i}; length: {t}, max score: {np.max(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
