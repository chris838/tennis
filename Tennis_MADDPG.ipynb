{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import pdb, pickle\n",
    "\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from holoviews import opts\n",
    "from holoviews.streams import Pipe, Buffer\n",
    "from holoviews.operation.timeseries import rolling\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "from maddpg_agent import MaddpgAgent\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"./Tennis.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default brain and reset env\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# Number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print(f\"Number of agents: {num_agents}\")\n",
    "\n",
    "# Size of the global state/action space (across all agents)\n",
    "actions = env_info.previous_vector_actions\n",
    "states = env_info.vector_observations\n",
    "global_state_space_size = states.flatten().shape[0]\n",
    "global_action_space_size = actions.flatten().shape[0]\n",
    "print(f\"Global states: {global_state_space_size}\")\n",
    "print(f\"Global actions: {global_action_space_size}\")\n",
    "\n",
    "# Size of the local state/action space (for each agent individually)\n",
    "action_space_size = brain.vector_action_space_size\n",
    "state_space_size = brain.num_stacked_vector_observations * brain.vector_observation_space_size\n",
    "print(f\"Local states: {state_space_size}\")\n",
    "print(f\"Local actions: {action_space_size}\")\n",
    "\n",
    "# Examine the state space \n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    num_episodes = 1500,\n",
    "    batch_size = 1024,\n",
    "    max_episode_length = 250,\n",
    "    train_after_batches = 20,\n",
    "    replay_buffer_size_max = int(1e6),\n",
    "    train_every_steps = 100,\n",
    "    noise_level = 2.0,\n",
    "    noise_decay = 0.9999,\n",
    "    print_episodes = 100\n",
    "):\n",
    "\n",
    "    print(f\"------------------------------------------------\")\n",
    "    print(f\"New training run.\")\n",
    "    print(f\"    num_episodes: {num_episodes}\")\n",
    "    print(f\"    batch_size: {batch_size}\")\n",
    "    print(f\"    max_episode_length: {max_episode_length}\")\n",
    "    print(f\"    train_after_batches: {train_after_batches}\")\n",
    "    print(f\"    replay_buffer_size_max: {replay_buffer_size_max}\")\n",
    "    print(f\"    train_every_steps: {train_every_steps}\")\n",
    "    print(f\"    noise_level: {noise_level}\")\n",
    "    print(f\"    noise_decay: {noise_decay}\")\n",
    "\n",
    "    # Create the agents\n",
    "    if agents is None:\n",
    "        agents = []\n",
    "        for i in range(num_agents):\n",
    "            print(f\"Agent {i}: state space: {state_space_size}; \\\n",
    "                    action space {action_space_size}.\")\n",
    "            agents.append(MaddpgAgent(\n",
    "                i, num_agents, state_space_size, action_space_size,\n",
    "                global_state_space_size, global_action_space_size))\n",
    "\n",
    "    # Don't start learning until we have a certain number of batches (obviously\n",
    "    # we need at least 1).\n",
    "    min_samples_required = batch_size * train_after_batches\n",
    "\n",
    "    # Create the replay buffer\n",
    "    if replay_buffer is None:\n",
    "        replay_buffer = ReplayBuffer(\n",
    "            max_size=replay_buffer_size_max, min_samples_required=min_samples_required)\n",
    "    \n",
    "    # Iterate over episodes\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        # Receive initial state vector s\n",
    "        #   s = (s_1, . . . , s_N)\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        s = env_info.vector_observations \n",
    "\n",
    "        episode_returns.append( np.array( [0] * num_agents) )\n",
    "        for t in range(1, max_episode_length):\n",
    "\n",
    "            # For each agent i, select actions:\n",
    "            #   a = (a_1, . . . , a_N)\n",
    "            # using the current policy and exploration noise, which we decay\n",
    "            a = [agent.act(state, noise_level=noise_level)\n",
    "                 for agent, state in zip(agents, s)]\n",
    "            if is_learning:\n",
    "                noise_level *= noise_decay\n",
    "\n",
    "            # Execute actions a = (a_1, . . . , a_N)\n",
    "            # Observe:\n",
    "            #   Reward r = (r_1, . . . , r_N)\n",
    "            #   Next-state vector s' = (s'_1, . . . , s'_N)\n",
    "            env_info= env.step(a)[brain_name]\n",
    "            r = env_info.rewards\n",
    "            s_prime = env_info.vector_observations\n",
    "            dones = env_info.local_done\n",
    "\n",
    "            # Store (s, a, r, s', done) in replay buffer\n",
    "            replay_buffer.append((s, a, r, s_prime, dones))\n",
    "\n",
    "            # Record progress\n",
    "            episode_returns[-1] = episode_returns[-1] + r\n",
    "\n",
    "            # Advance\n",
    "            s = s_prime\n",
    "            train_step += 1\n",
    "\n",
    "            # Periodically (after a certain number of steps) run update/training\n",
    "            if train_step % train_every_steps == 0:\n",
    "                if replay_buffer.has_enough_samples():\n",
    "                    \n",
    "                    if not is_learning:\n",
    "                        print(f\"Started learning at time {train_step}\")\n",
    "                        is_learning = True\n",
    "\n",
    "                    # Sample replay buffer\n",
    "                    sample = replay_buffer.sample(batch_size=batch_size)\n",
    "\n",
    "                    # For every sample tuple, each agent needs to know which action\n",
    "                    # would be chosen under the policy of the other agents in the\n",
    "                    # next state s', in order to calculate q-values.\n",
    "                    next_actions = [[\n",
    "                         agent.act(next_state, target_actor=True)\n",
    "                         for agent, next_state in zip(agents, s_prime)]\n",
    "                        for (s, a, r, s_prime, dones) in sample]\n",
    "                    \n",
    "                    # Update/train all the agents\n",
    "                    per_agent_loss = []\n",
    "                    for agent in agents:\n",
    "                        actor_loss, critic_loss = agent.update(sample, next_actions)\n",
    "                        per_agent_loss.append((actor_loss, critic_loss))\n",
    "                    loss.append(per_agent_loss)\n",
    "                    \n",
    "            # Terminate episode early if done\n",
    "            if any(dones):\n",
    "                break\n",
    "\n",
    "        if episode % print_episodes == 0:\n",
    "            print(f\"t: {train_step}, e: {episode}, noise: {noise_level:.2f}. \" + \\\n",
    "                  f\"Average last {print_episodes} episode return: \" + \\\n",
    "                  f\"{np.array(episode_returns[-print_episodes:]).mean(axis=0)}\")\n",
    "            \n",
    "    print(f\"Final average reward over entire run: {np.mean(episode_returns)}\")\n",
    "    if len(loss) > 0:\n",
    "        average_loss = np.array(loss).mean(axis=1)\n",
    "    else :\n",
    "        average_loss = None\n",
    "    return np.array(episode_returns).sum(axis=1), average_loss, agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "loss = []\n",
    "agents = []\n",
    "replay_buffer = None\n",
    "train_step = 0\n",
    "is_learning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    num_episodes=30000,\n",
    "    max_episode_length = 2000,\n",
    "    batch_size=512,\n",
    "    train_every_steps=4,\n",
    "    train_after_batches=40,\n",
    "    noise_level = 1.0,\n",
    "    noise_decay = 0.9999,\n",
    "    print_episodes = 50\n",
    ")\n",
    "\n",
    "average_loss = np.array(loss).mean(axis=1)\n",
    "average_return = np.array(episode_returns).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_curves = hv.Curve(results) * rolling(hv.Curve(results), rolling_window=100)\n",
    "loss_curves = hv.Curve(loss[:,0]).relabel('actor_loss') * hv.Curve(loss[:,1]).relabel('critic_loss')\n",
    "(results_curves + loss_curves).opts(opts.Curve(axiswise=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 15):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    t = 0\n",
    "    while True:\n",
    "        t += 1\n",
    "        \n",
    "        #actions = np.random.randn(num_agents, action_space_size)\n",
    "        \n",
    "        actions = [agent.act(state, noise_level=1) for agent, state in zip(agents, states)]\n",
    "        \n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print(f'Episode: {i}; length: {t}, max score: {np.max(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a grid search in hyperparam space\n",
    "\n",
    "coords = {}\n",
    "\n",
    "num_episodes = 1500\n",
    "batch_sizes = [32, 64, 256, 1024]\n",
    "train_everys = [1, 10, 50, 100]\n",
    "\n",
    "results_1 = []\n",
    "for batch_size in batch_sizes:\n",
    "    \n",
    "    results_2 = []\n",
    "    for train_every in train_everys:\n",
    "        \n",
    "        label = f\"Batch size: {batch_size}; Train every: {train_every}\"\n",
    "        results = train(\n",
    "            num_episodes=num_episodes,\n",
    "            batch_size=batch_size,\n",
    "            train_every_steps=train_every)\n",
    "        \n",
    "        results_2.append(results)\n",
    "    \n",
    "    results_1.append(results_2)\n",
    "    \n",
    "    pickle_out = open(\"results_1.pickle\",\"wb\")\n",
    "    pickle.dump(results_1, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the grid search output graphically\n",
    "xr_results = xr.DataArray(results_1,\n",
    "             coords={'batch_size':batch_sizes, 'train_every':train_everys, 'episode_index':range(1,num_episodes)},\n",
    "             dims=['batch_size', 'train_every', 'episode_index'])\n",
    "hv_results = hv.Dataset(xr_results, ['batch_size', 'train_every', 'episode_index'], 'reward')\n",
    "raw_grid = hv_results.to(hv.Curve, ['episode_index']).grid()\n",
    "smooth_grid = rolling(hv_results.to(hv.Curve, ['episode_index']), rolling_window=100).grid()\n",
    "raw_grid * smooth_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
