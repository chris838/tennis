{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "from maddpg_agent import MaddpgAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"./Tennis.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default brain and reset env\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# Number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print(f\"Number of agents: {num_agents}\")\n",
    "\n",
    "# Size of the global state/action space (across all agents)\n",
    "actions = env_info.previous_vector_actions\n",
    "states = env_info.vector_observations\n",
    "global_state_space_size = states.flatten().shape[0]\n",
    "global_action_space_size = actions.flatten().shape[0]\n",
    "print(f\"Global states: {global_state_space_size}\")\n",
    "print(f\"Global actions: {global_action_space_size}\")\n",
    "\n",
    "# Size of the local state/action space (for each agent individually)\n",
    "action_space_size = brain.vector_action_space_size\n",
    "state_space_size = brain.num_stacked_vector_observations * brain.vector_observation_space_size\n",
    "print(f\"Local states: {state_space_size}\")\n",
    "print(f\"Local actions: {action_space_size}\")\n",
    "\n",
    "# Examine the state space \n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "num_episodes = 1000\n",
    "batch_size = 1024  # how many episodes to process at once\n",
    "max_episode_length = 25\n",
    "replay_buffer_size_max = int(1e6)\n",
    "train_every_steps = 100  # steps between training updates\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agents\n",
    "agents = []\n",
    "for i in range(num_agents):\n",
    "    print(f\"Agent {i}: state space: {state_space_size}; \\\n",
    "            action space {action_space_size}.\")\n",
    "    agents.append(MaddpgAgent(\n",
    "        i, num_agents, state_space_size, action_space_size,\n",
    "        global_state_space_size, global_action_space_size))\n",
    "\n",
    "# Don't start learning until we have more episodes recorded than we need\n",
    "# samples to fill our batch (i.e. we're only taking on average 1-2 samples from\n",
    "# each episode).\n",
    "min_samples_required = batch_size * max_episode_length\n",
    "\n",
    "# Create the replay buffer\n",
    "replay_buffer = ReplayBuffer(\n",
    "    max_size=replay_buffer_size_max, min_samples_required=min_samples_required)\n",
    "\n",
    "# Track progress\n",
    "episode_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over episodes\n",
    "train_step = 0\n",
    "for episode in range(1, num_episodes):\n",
    "\n",
    "    # Receive initial state vector s\n",
    "    #   s = (s_1, . . . , s_N)\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    s = env_info.vector_observations \n",
    "\n",
    "    episode_rewards.append( np.array( [0] * num_agents) )\n",
    "    for t in range(1, max_episode_length):\n",
    "\n",
    "        # For each agent i, select actions:\n",
    "        #   a = (a_1, . . . , a_N)\n",
    "        # using the current policy and exploration noise, which we decay\n",
    "        a = [agent.act(state, epsilon=epsilon)\n",
    "             for agent, state in zip(agents, s)]\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "        # Execute actions a = (a_1, . . . , a_N)\n",
    "        # Observe:\n",
    "        #   Reward r = (r_1, . . . , r_N)\n",
    "        #   Next-state vector s' = (s'_1, . . . , s'_N)\n",
    "        env_info= env.step(a)[brain_name]\n",
    "        s_prime = env_info.vector_observations\n",
    "        r = env_info.rewards\n",
    "\n",
    "        # Store (s, a, r, s') in replay buffer D\n",
    "        replay_buffer.append((s, a, r, s_prime))\n",
    "\n",
    "        # Record progress\n",
    "        episode_rewards[-1] = episode_rewards[-1] + r\n",
    "\n",
    "        # Advance\n",
    "        s = s_prime\n",
    "        train_step += 1\n",
    "\n",
    "        # Periodically (after a certain number of steps) run update/training\n",
    "        if train_step % train_every_steps == 0:\n",
    "            if replay_buffer.has_enough_samples():\n",
    "\n",
    "                # Sample replay buffer\n",
    "                sample = replay_buffer.sample(batch_size=batch_size)\n",
    "\n",
    "                # For every sample tuple, each agent needs to know which action\n",
    "                # would be chosen under the policy of the other agents in the\n",
    "                # next state s', in order to calculate q-values.\n",
    "                next_actions = [[\n",
    "                     agent.act(next_state, target_actor=True)\n",
    "                     for agent, next_state in zip(agents, s_prime)]\n",
    "                    for (s, a, r, s_prime) in sample]\n",
    "\n",
    "                # Update/train all the agents\n",
    "                for agent in agents:\n",
    "                    agent.update(sample, next_actions)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Average episode return over last 100 episodes: \\\n",
    "        {np.array(episode_rewards[-100:]).mean(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
