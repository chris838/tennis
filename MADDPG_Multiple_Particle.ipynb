{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Particle Environments\n",
    "\n",
    "Testing my MADDPG solution on Multi-Agent Particle Environments.\n",
    "\n",
    "Environments were take from [github.com/openai/multiagent-particle-envs](https://github.com/openai/multiagent-particle-envs).\n",
    "\n",
    "MADDPG agent implementation based on [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "\n",
    "from mp_envs import make_env\n",
    "from replay_buffer import ReplayBuffer\n",
    "from maddpg_agent import MaddpgAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradient for N agents\n",
    "\n",
    "# Config\n",
    "num_episodes = 5000\n",
    "batch_size = 1024  # how many episodes to process at once\n",
    "max_episode_length = 25\n",
    "environment_name = \"simple_adversary\"\n",
    "replay_buffer_size_max = int(1e6)\n",
    "train_every_steps = 100  # steps between training updates\n",
    "noise_level = 1.0\n",
    "noise_decay = 0.9999\n",
    "\n",
    "# Load the environment\n",
    "env = make_env(environment_name)\n",
    "num_agents = env.n\n",
    "print(f\"Environment has {num_agents} agents\")\n",
    "\n",
    "# Create the agents\n",
    "agents = []\n",
    "global_state_space_size = sum([o.shape[0] for o in env.observation_space])\n",
    "global_action_space_size = sum([a.shape[0] for a in env.action_space])\n",
    "for i in range(num_agents):\n",
    "    state_space_size = env.observation_space[i].shape[0]\n",
    "    action_space_size = env.action_space[i].shape[0]\n",
    "    print(f\"Agent {i}: state space: {state_space_size}; \\\n",
    "            action space {action_space_size}.\")\n",
    "    agents.append(MaddpgAgent(\n",
    "        i, num_agents, state_space_size, action_space_size,\n",
    "        global_state_space_size, global_action_space_size))\n",
    "\n",
    "# Don't start learning until we have more episodes recorded than we need\n",
    "# samples to fill our batch (i.e. we're only taking on average 1-2 samples from\n",
    "# each episode).\n",
    "min_samples_required = batch_size * max_episode_length\n",
    "\n",
    "# Create the replay buffer\n",
    "replay_buffer = ReplayBuffer(\n",
    "    max_size=replay_buffer_size_max, min_samples_required=min_samples_required)\n",
    "\n",
    "# Track progress\n",
    "episode_rewards = []\n",
    "\n",
    "# Iterate over episodes\n",
    "train_step = 0\n",
    "is_learning = False\n",
    "for episode in range(1, num_episodes):\n",
    "\n",
    "    # Receive initial state vector s\n",
    "    #   s = (s_1, . . . , s_N)\n",
    "    s = env.reset()\n",
    "\n",
    "    episode_rewards.append( np.array( [0] * num_agents) )\n",
    "    for t in range(1, max_episode_length):\n",
    "\n",
    "        # For each agent i, select actions:\n",
    "        #   a = (a_1, . . . , a_N)\n",
    "        # using the current policy and exploration noise, which we decay\n",
    "        a = [agent.act(state, noise_level=noise_level)\n",
    "             for agent, state in zip(agents, s)]\n",
    "        if is_learning:\n",
    "            noise_level *= noise_decay\n",
    "\n",
    "        # Execute actions a = (a_1, . . . , a_N)\n",
    "        # Observe:\n",
    "        #   Reward r = (r_1, . . . , r_N)\n",
    "        #   Next-state vector s' = (s'_1, . . . , s'_N)\n",
    "        s_prime, r, *_ = env.step(a)\n",
    "\n",
    "        # Store (s, a, r, s') in replay buffer D\n",
    "        replay_buffer.append((s, a, r, s_prime))\n",
    "\n",
    "        # Record progress\n",
    "        episode_rewards[-1] = episode_rewards[-1] + r\n",
    "\n",
    "        # Advance\n",
    "        s = s_prime\n",
    "        train_step += 1\n",
    "\n",
    "        # Periodically (after a certain number of steps) run update/training\n",
    "        if train_step % train_every_steps == 0:\n",
    "            if replay_buffer.has_enough_samples():\n",
    "\n",
    "                if not is_learning:\n",
    "                    print(f\"Started learning at time {train_step}\")\n",
    "                    is_learning = True\n",
    "\n",
    "                # Sample replay buffer\n",
    "                sample = replay_buffer.sample(batch_size=batch_size)\n",
    "\n",
    "                # For every sample tuple, each agent needs to know which action\n",
    "                # would be chosen under the policy of the other agents in the\n",
    "                # next state s', in order to calculate q-values.\n",
    "                next_actions = [[\n",
    "                     agent.act(next_state, target_actor=True)\n",
    "                     for agent, next_state in zip(agents, s_prime)]\n",
    "                    for (s, a, r, s_prime) in sample]\n",
    "\n",
    "                # Update/train all the agents\n",
    "                for agent in agents:\n",
    "                    agent.update(sample, next_actions)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"t: {train_step}, e: {episode}, noise: {noise_level:.2f}. Average last 100 episode return: \\\n",
    "        {np.array(episode_rewards[-100:]).mean(axis=0)}\")\n",
    "\n",
    "print(f\"Final average reward over entire run: {np.mean(episode_rewards)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
